{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"U2-net_Py-Custom.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1b1JTtYESZhitbPOjuLaSSvo9uI2qSrP-","authorship_tag":"ABX9TyPQ1jLSDpdgjgKb69mgSHQv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Computer Vision (Coding L2)\n","\n","This aim of this challenge to assess your skills in applying creative and quantitative computer vision and deep learning techniques to measuring the location and diameters of a subject’s pupils and iris from a cropped eye image.\n","\n","Included here are 500 random images from our Training dataset:\n","\n","https://www.dropbox.com/sh/wsxpcjcr0f4exg4/AAC8yyF0Q8qFHLmvw07QlAcUa?dl=0\n","\n","\n","The challenge is specifically as follows:\n","- Estimate the average diameter (in pixels) of both a) the pupil and b) the iris in each image.\n","- Alternatively, you can treat these objects as ellipses and estimate the major and minor axes.\n","- Train a deep learning model with these images and demonstrate the model’s performance on the testing images provided in the link.\n","- The same approach must be applied to each image.\n","- You may choose to implement any state-of-the-art model and may also use pre-trained weights to speed up the training process.\n","- We are interested in seeing how different your approach is from an existing off-the-shelf model, catering to our problem.\n","- We will go through the code to understand your implementation in depth and to get a sense of your coding style.\n","- The challenge will be assessed by maximizing accuracy of the inferred diameter across the provided testing images.\n","- Due: Within 7 days from this email.\n","\n","\n","### Notes:\n","The provided mask images contain 3 channels. You may select any channel to access pupil and iris co-ordinates. In any selected channel, pixel values with '0' correspond to the background class, pixels with value '1' correspond to the pupil and '2' refers to the iris class.\n","\n","### Metrics to implement (6 in total):\n","\n","• Mean % Pupil diameter error = Absolute(Predicted_pupil_diameter – Groundtruth_pupil_diameter) / Groundtruth_pupil_diameter * 100\n","[2 in total: using radiusY and radius for the pupil]\n","\n","• Mean % Iris diameter error = Absolute(Predicted_iris_diameter – Groundtruth_iris_diameter) / Groundtruth_iris_diameter * 100\n","[2 in total: using radiusY and radius for the iris]\n","\n","• Mean Pupil IoU (0 to 1)\n","\n","• Mean Iris IoU (0 to 1)\n","\n","An example of a cropped eye image and its’ corresponding segmentation mask is shown below.\n","\n","##  Submission should include a zip file containing the following:\n","\n","1. Well-commented Python script file or Jupyter Notebook file(.py or .ipynb)\n","2. If using just a Python script file (.py), please include a document (.docx or .pdf) explaining your approach in brief\n","3. Please mention your reasoning for different decisions you made in the code comments or the attached document\n","4. You may use any DL framework from Keras/Tensorflow/PyTorch, although we would prefer PyTorch\n","5. Trained weights of the model architecture you used/implemented\n","6. Evaluation code to easily test out the trained model on our internal testing dataset\n","7. Metric results of your code on the provided testing set\n","\n","PS: If the size of the zip file is larger than the allowable size limit on DropBox, please upload your submission in a shareable folder and attach the link to your code + model into the document attached to the response."],"metadata":{"id":"SdRoOELjfSFF"}},{"cell_type":"code","source":["!pip install git+https://github.com/qubvel/segmentation_models.pytorch"],"metadata":{"id":"dPN-9Q60KVAT"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UwhqeW1cekAp"},"outputs":[],"source":["import torch\n","import cv2\n","from pathlib import Path\n","from torch.utils import data\n","import time\n","\n","from __future__ import print_function, division\n","import os\n","import torch\n","import pandas as pd\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import segmentation_models_pytorch as smp\n","\n","from torchvision import transforms, utils\n","\n","# Ignore warnings\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","plt.ion()   # interactive mode\n","proj_path = Path('/content/drive/MyDrive/Colab Notebooks/Code Tasks/Segmentation')"]},{"cell_type":"code","source":["# create fake test data\n","x = np.random.randint(0, 256, size=(128, 128, 3), dtype=np.uint8)\n","y = np.random.randint(0, 1, size=(128, 128), dtype=np.uint8)\n","cv2.imwrite(str(proj_path/\"Sample Data\"/\"img_x.png\"), x)\n","cv2.imwrite(str(proj_path/\"Sample Data\"/\"mask_y.png\"), y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4WKoHd7TO8YK","executionInfo":{"status":"ok","timestamp":1643321531018,"user_tz":300,"elapsed":1989,"user":{"displayName":"Matt M","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgGWGZJr0AvvKDbLZhbB9MWR9w00xXd1E5qIrO68vg=s64","userId":"15935538921989259052"}},"outputId":"5f0011e9-4492-4520-db37-85bce15cc95a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["class dataset_segmentation(data.Dataset):\n","    \"\"\"Image segmentation dataset with caching, pretransforms and multiprocessing. Output is a dict.\"\"\"\n","\n","    def __init__(self, inputs: list, targets: list, transform=None, use_cache=False, pre_transform=None):\n","        self.inputs = inputs\n","        self.targets = targets\n","        self.transform = transform\n","        self.inputs_dtype = torch.float32\n","        self.targets_dtype = torch.long\n","        self.use_cache = use_cache\n","        self.pre_transform = pre_transform\n","\n","        if self.use_cache:\n","            from itertools import repeat\n","            from multiprocessing import Pool\n","\n","            with Pool() as pool:\n","                self.cached_data = pool.starmap(\n","                    self.read_images, zip(inputs, targets, repeat(self.pre_transform)))\n","\n","    def __len__(self):\n","        return len(self.inputs)\n","\n","    def __getitem__(self, index: int):\n","        if self.use_cache:\n","            x, y = self.cached_data[index]\n","        else:\n","            # Select the sample\n","            input_ID = self.inputs[index]\n","            target_ID = self.targets[index]\n","\n","            # Load input and target\n","            x, y = cv2.imread(str(input_ID)), cv2.imread(str(target_ID))\n","            if x is None or y is None: raise Exception(\"File not found\")\n","\n","        # Preprocessing\n","        if self.transform is not None: x, y = self.transform(x, y)\n","\n","        # Typecasting\n","        x, y = torch.from_numpy(x).type(self.inputs_dtype), torch.from_numpy(y).type(\n","            self.targets_dtype\n","        )\n","\n","        return {\n","            \"x\": x,\n","            \"y\": y,\n","            \"x_name\": self.inputs[index].name,\n","            \"y_name\": self.targets[index].name,\n","        }\n","\n","    @staticmethod\n","    def read_images(inp, tar, pre_transform):\n","        inp, tar = cv2.imread(str(inp)), cv2.imread(str(tar))\n","        if pre_transform:\n","            inp, tar = pre_transform(inp, tar)\n","        return inp, tar\n"],"metadata":{"id":"C0POk3pen_x6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["inputs = [proj_path/\"Sample Data\"/\"img_x.png\"]\n","targets = [proj_path/\"Sample Data\"/\"mask_y.png\"]\n","\n","training_dataset = dataset_segmentation(inputs=inputs, targets=targets, transform=None)\n","\n","training_dataloader = data.DataLoader(dataset=training_dataset, batch_size=1, shuffle=True)\n","\n","example = next(iter(training_dataloader))\n","x, y = example['x'], example['y']\n","print(f'x = shape: {x.shape}; type: {x.dtype}')\n","print(f'x = min: {x.min()}; max: {x.max()}')\n","print(f'y = shape: {y.shape}; class: {y.unique()}; type: {y.dtype}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rtXccr3MGoPA","executionInfo":{"status":"ok","timestamp":1643321674982,"user_tz":300,"elapsed":231,"user":{"displayName":"Matt M","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgGWGZJr0AvvKDbLZhbB9MWR9w00xXd1E5qIrO68vg=s64","userId":"15935538921989259052"}},"outputId":"8af86ef2-a5fa-4dc2-e3b9-6c7daa0e3a91"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["x = shape: torch.Size([1, 128, 128, 3]); type: torch.float32\n","x = min: 0.0; max: 255.0\n","y = shape: torch.Size([1, 128, 128, 3]); class: tensor([0]); type: torch.int64\n"]}]},{"cell_type":"markdown","source":["Data Augmentation"],"metadata":{"id":"YZoB02_pzPZv"}},{"cell_type":"code","source":["import torchvision.transforms.functional as TF\n","data_transformations = {\n","    'train': transforms.Compose([\n","        transforms.RandomResizedCrop(224),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.RandomVerticalFlip(),\n","        transforms.ToTensor(),\n","    ]),\n","    'val': transforms.Compose([\n","        transforms.Resize(256),\n","        transforms.CenterCrop(224),\n","        transforms.ToTensor(),\n","    ]),\n","}"],"metadata":{"id":"FeiXxOJPzRVK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# dataset training\n","dataset_train = dataset_segmentation(inputs=inputs_train,\n","                                    targets=targets_train,\n","                                    transform=data_transformations)\n","\n","# dataset validation\n","dataset_valid = dataset_segmentation(inputs=inputs_valid,\n","                                    targets=targets_valid,\n","                                    transform=data_transformations)\n","\n","# dataloader training\n","dataloader_training = data.DataLoader(dataset=dataset_train,\n","                                 batch_size=2,\n","                                 shuffle=True)\n","\n","# dataloader validation\n","dataloader_validation = data.DataLoader(dataset=dataset_valid,\n","                                   batch_size=2,\n","                                   shuffle=True)"],"metadata":{"id":"si_MRT1LMFeL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Training"],"metadata":{"id":"q3v3bU6JLHVv"}},{"cell_type":"code","source":["def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n","    since = time.time()\n","\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_acc = 0.0\n","\n","    for epoch in range(num_epochs):\n","        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n","        print('-' * 10)\n","\n","        # Each epoch has a training and validation phase\n","        for phase in ['train', 'val']:\n","            if phase == 'train':\n","                model.train()  # Set model to training mode\n","            else:\n","                model.eval()   # Set model to evaluate mode\n","\n","            running_loss = 0.0\n","            running_corrects = 0\n","\n","            # Iterate over data.\n","            for inputs, labels in dataloaders[phase]:\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)\n","\n","                # zero the parameter gradients\n","                optimizer.zero_grad()\n","\n","                # forward\n","                # track history if only in train\n","                with torch.set_grad_enabled(phase == 'train'):\n","                    outputs = model(inputs)\n","                    _, preds = torch.max(outputs, 1)\n","                    loss = criterion(outputs, labels)\n","\n","                    # backward + optimize only if in training phase\n","                    if phase == 'train':\n","                        loss.backward()\n","                        optimizer.step()\n","\n","                # statistics\n","                running_loss += loss.item() * inputs.size(0)\n","                running_corrects += torch.sum(preds == labels.data)\n","            if phase == 'train':\n","                scheduler.step()\n","\n","            epoch_loss = running_loss / dataset_sizes[phase]\n","            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n","\n","            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n","                phase, epoch_loss, epoch_acc))\n","\n","            # deep copy the model\n","            if phase == 'val' and epoch_acc > best_acc:\n","                best_acc = epoch_acc\n","                best_model_wts = copy.deepcopy(model.state_dict())\n","\n","        print()\n","\n","    time_elapsed = time.time() - since\n","    print('Training complete in {:.0f}m {:.0f}s'.format(\n","        time_elapsed // 60, time_elapsed % 60))\n","    print('Best val Acc: {:4f}'.format(best_acc))\n","\n","    # load best model weights\n","    model.load_state_dict(best_model_wts)\n","    return model"],"metadata":{"id":"L3g6ZweGLHxK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Inference"],"metadata":{"id":"zH1eP3Nld4nJ"}},{"cell_type":"code","source":["# Imports\n","import pathlib\n","\n","import numpy as np\n","import torch\n","from skimage.io import imread\n","from skimage.transform import resize\n","\n","#from inference import predict\n","#from unet import UNet\n","\n","# root directory\n","root = pathlib.Path.cwd() / 'Carvana' / 'Test'\n","def get_filenames_of_path(path: pathlib.Path, ext: str = '*'):\n","    \"\"\"Returns a list of files in a directory/path. Uses pathlib.\"\"\"\n","    filenames = [file for file in path.glob(ext) if file.is_file()]\n","    return filenames\n","\n","# input and target files\n","images_names = get_filenames_of_path(root / 'Input')\n","targets_names = get_filenames_of_path(root / 'Target')\n","\n","# read images and store them in memory\n","images = [imread(img_name) for img_name in images_names]\n","targets = [imread(tar_name) for tar_name in targets_names]\n","\n","# Resize images and targets\n","images_res = [resize(img, (128, 128, 3)) for img in images]\n","resize_kwargs = {'order': 0, 'anti_aliasing': False, 'preserve_range': True}\n","targets_res = [resize(tar, (128, 128), **resize_kwargs) for tar in targets]\n","\n","# device\n","if torch.cuda.is_available():\n","    device = torch.device('cuda')\n","else:\n","    torch.device('cpu')\n","\n","# model\n","model = UNet(in_channels=3,\n","             out_channels=2,\n","             n_blocks=4,\n","             start_filters=32,\n","             activation='relu',\n","             normalization='batch',\n","             conv_mode='same',\n","             dim=2).to(device)\n","\n","\n","model_name = 'carvana_model.pt'\n","model_weights = torch.load(pathlib.Path.cwd() / model_name)\n","\n","model.load_state_dict(model_weights)\n","\n","\n","# predict the segmentation maps \n","output = [predict(img, model, preprocess, postprocess, device) for img in images_res]"],"metadata":{"id":"Zi7aBMB9pL1X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Analysis"],"metadata":{"id":"so7SZCxmd7Sf"}},{"cell_type":"code","source":["\n","# Load image (as BGR for later drawing the circle)\n","image = cv2.imread('images/hvFJF.jpg', cv2.IMREAD_COLOR)\n","\n","# Convert to grayscale\n","gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n","\n","# Get rid of possible JPG artifacts (when do people learn to use PNG?...)\n","_, gray = cv2.threshold(gray, 128, 255, cv2.THRESH_BINARY)\n","\n","# Downsize image (by factor 4) to speed up morphological operations\n","gray = cv2.resize(gray, dsize=(0, 0), fx=0.25, fy=0.25)\n","\n","# Morphological Closing: Get rid of the hole\n","gray = cv2.morphologyEx(gray, cv2.MORPH_CLOSE, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5)))\n","\n","# Morphological opening: Get rid of the stuff at the top of the circle\n","gray = cv2.morphologyEx(gray, cv2.MORPH_OPEN, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (121, 121)))\n","\n","# Resize image to original size\n","gray = cv2.resize(gray, dsize=(image.shape[1], image.shape[0]))\n","\n","# Find contours (only most external)\n","cnts, _ = cv2.findContours(gray, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n","\n","# Draw found contour(s) in input image\n","image = cv2.drawContours(image, cnts, -1, (0, 0, 255), 2)\n","\n","cv2.imwrite('images/intermediate.png', gray)\n","cv2.imwrite('images/result.png', image)"],"metadata":{"id":"Pe-BYvW9d9WR"},"execution_count":null,"outputs":[]}]}